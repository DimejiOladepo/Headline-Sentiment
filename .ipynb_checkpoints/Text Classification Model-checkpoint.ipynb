{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "data = \"the_news_headline_labeled_data.csv\"\n",
    "news_headlines = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"Mexican presidential front-runner hits back ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b\"Cramer reflects on how Trump's actions are f...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'The Wall Street Journal: Peter Thiels VC fir...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'WSJ Wealth Adviser Briefing: Catchy Tickers,...</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'Apple proved that it is no longer just an iP...</td>\n",
       "      <td>Even</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT     LABEL\n",
       "0  b\"Mexican presidential front-runner hits back ...  Negative\n",
       "1  b\"Cramer reflects on how Trump's actions are f...  Positive\n",
       "2  b'The Wall Street Journal: Peter Thiels VC fir...  Positive\n",
       "3  b'WSJ Wealth Adviser Briefing: Catchy Tickers,...      Even\n",
       "4  b'Apple proved that it is no longer just an iP...      Even"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22af1359f60>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Label Pie chart \n",
    "plot_size = plt.rcParams[\"figure.figsize\"]\n",
    "\n",
    "plot_size[0] = 5\n",
    "plot_size[1] = 5\n",
    "plt.rcParams[\"figure.figsize\"] = plot_size\n",
    "\n",
    "news_headlines.LABEL.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"yellow\", \"red\", \"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating feature and label set\n",
    "features = news_headlines.iloc[:,0].values\n",
    "labels = news_headlines.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing and cleaning the dataset using regular expressions\n",
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming labels strings to categorical integers and splitting the dataset into training and test sets without conversion \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "Validation_split = 0.2\n",
    "\n",
    "num_validation_samples = int(Validation_split * np.asarray(processed_features).shape[0])\n",
    "\n",
    "x_train = processed_features[:-num_validation_samples]\n",
    "x_test = processed_features[-num_validation_samples:]\n",
    "Y_train = lb_make.fit_transform(labels[:-num_validation_samples])\n",
    "Y_test = lb_make.fit_transform(labels[-num_validation_samples:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oladimeji\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Learning Embeddings \n",
    "# Tokenizing sentences using Keras  \n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(processed_feature)\n",
    "\n",
    "#Determining number of words in largest sentence and vocabulary size of the corpus\n",
    "max_length = max([len(sentence.split()) for sentence in processed_feature])\n",
    "\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sentences in train and test sets to sequences\n",
    "x_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
    "\n",
    "#Padding to make sure all the vectors are the same length \n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen= max_length, padding= 'post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen= max_length, padding= 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\Oladimeji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Oladimeji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries for deep neural network\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "#Defining neural network using embedding layer as hidden layer with random weights initialized.\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length))\n",
    "model.add(GRU(units=32, dropout= 0.2, recurrent_dropout= 0.2))\n",
    "model.add(Dense(1, activation= 'relu'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 100)            2300      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,101\n",
      "Trainable params: 15,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Oladimeji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      " - 6s - loss: 2.4403 - acc: 0.4625 - val_loss: 1.4831 - val_acc: 0.4800\n",
      "Epoch 2/25\n",
      " - 0s - loss: 1.4438 - acc: 0.4625 - val_loss: 1.1728 - val_acc: 0.4800\n",
      "Epoch 3/25\n",
      " - 0s - loss: 1.1795 - acc: 0.4625 - val_loss: 0.9872 - val_acc: 0.4800\n",
      "Epoch 4/25\n",
      " - 0s - loss: 0.9898 - acc: 0.4625 - val_loss: 0.8452 - val_acc: 0.4800\n",
      "Epoch 5/25\n",
      " - 0s - loss: 0.8273 - acc: 0.4550 - val_loss: 0.7384 - val_acc: 0.4800\n",
      "Epoch 6/25\n",
      " - 0s - loss: 0.7373 - acc: 0.3150 - val_loss: 0.6629 - val_acc: 0.3200\n",
      "Epoch 7/25\n",
      " - 0s - loss: 0.6486 - acc: 0.3038 - val_loss: 0.6295 - val_acc: 0.3350\n",
      "Epoch 8/25\n",
      " - 0s - loss: 0.6013 - acc: 0.3038 - val_loss: 0.6199 - val_acc: 0.3350\n",
      "Epoch 9/25\n",
      " - 0s - loss: 0.6002 - acc: 0.3050 - val_loss: 0.6168 - val_acc: 0.3350\n",
      "Epoch 10/25\n",
      " - 0s - loss: 0.5779 - acc: 0.3050 - val_loss: 0.6024 - val_acc: 0.3350\n",
      "Epoch 11/25\n",
      " - 0s - loss: 0.5756 - acc: 0.3050 - val_loss: 0.5930 - val_acc: 0.3350\n",
      "Epoch 12/25\n",
      " - 0s - loss: 0.5650 - acc: 0.3050 - val_loss: 0.5878 - val_acc: 0.3350\n",
      "Epoch 13/25\n",
      " - 0s - loss: 0.5606 - acc: 0.3063 - val_loss: 0.5810 - val_acc: 0.3350\n",
      "Epoch 14/25\n",
      " - 0s - loss: 0.5628 - acc: 0.3063 - val_loss: 0.5751 - val_acc: 0.3350\n",
      "Epoch 15/25\n",
      " - 0s - loss: 0.5611 - acc: 0.3088 - val_loss: 0.5739 - val_acc: 0.3350\n",
      "Epoch 16/25\n",
      " - 0s - loss: 0.5728 - acc: 0.3038 - val_loss: 0.5705 - val_acc: 0.3350\n",
      "Epoch 17/25\n",
      " - 0s - loss: 0.5423 - acc: 0.3063 - val_loss: 0.5735 - val_acc: 0.3300\n",
      "Epoch 18/25\n",
      " - 0s - loss: 0.5572 - acc: 0.3075 - val_loss: 0.4997 - val_acc: 0.3250\n",
      "Epoch 19/25\n",
      " - 0s - loss: 0.5126 - acc: 0.3000 - val_loss: 0.4983 - val_acc: 0.3250\n",
      "Epoch 20/25\n",
      " - 0s - loss: 0.5165 - acc: 0.3038 - val_loss: 0.4994 - val_acc: 0.3250\n",
      "Epoch 21/25\n",
      " - 0s - loss: 0.5117 - acc: 0.3063 - val_loss: 0.5048 - val_acc: 0.3250\n",
      "Epoch 22/25\n",
      " - 0s - loss: 0.5021 - acc: 0.3050 - val_loss: 0.5056 - val_acc: 0.3250\n",
      "Epoch 23/25\n",
      " - 0s - loss: 0.5083 - acc: 0.3038 - val_loss: 0.5066 - val_acc: 0.3250\n",
      "Epoch 24/25\n",
      " - 0s - loss: 0.4985 - acc: 0.3025 - val_loss: 0.5036 - val_acc: 0.3250\n",
      "Epoch 25/25\n",
      " - 0s - loss: 0.4680 - acc: 0.3038 - val_loss: 0.5062 - val_acc: 0.3250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22ae95f0908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model on the training set and cross-validating on test set \n",
    "model.fit(x_train_pad, Y_train, batch_size=64, epochs= 25, validation_data=(x_test_pad, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to improve using word2vec model. It separately learns word embeddings and then passes to the embedding layer\n",
    "#importing word tokenize and stopwords from nltk library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "headlines = list()\n",
    "lines = list(processed_features)\n",
    "\n",
    "#Creating word tokens, removing stopwords and listing the tokens\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tokens if not word in stop_words]\n",
    "    headlines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oladimeji\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "#Importing gensim to create embeddings\n",
    "import gensim\n",
    "\n",
    "#Creating model through Gensim's Word2Vec API and initializing parameters\n",
    "model_gen = gensim.models.Word2Vec(sentences= headlines, size= EMBEDDING_DIM, window= 5, workers= 4, min_count= 1)\n",
    "\n",
    "#Creating vocabulary list\n",
    "vocabulary = list(model_gen.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Word2Vec Vocabulary size is: 3392\n"
     ]
    }
   ],
   "source": [
    "print(\"The Word2Vec Vocabulary size is:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frills', 0.349689781665802),\n",
       " ('age', 0.3082854449748993),\n",
       " ('handling', 0.3033483326435089),\n",
       " ('31', 0.29810279607772827),\n",
       " ('assistant', 0.29647305607795715),\n",
       " ('make', 0.28357571363449097),\n",
       " ('cocktail', 0.2736015319824219),\n",
       " ('heist', 0.27144578099250793),\n",
       " ('advantage', 0.2681414783000946),\n",
       " ('administration', 0.2663113474845886)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing for synonyms in the embedding\n",
    "model_gen.wv.most_similar('mexican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model for use\n",
    "filename = 'news_headlines_embedding_word2vec.txt'\n",
    "model_gen.wv.save_word2vec_format(filename, binary= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the word2vec model \n",
    "import os \n",
    "\n",
    "embeddings_index = {}\n",
    "file = open(os.path.join('', 'news_headlines_embedding_word2vec.txt'), encoding= \"utf-8\")\n",
    "\n",
    "#Creating coefficients for the first word in the sentence\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3392 unique tokens.\n",
      "Shape of headline Tensor: (1000, 1)\n",
      "Shape of label: (1000,)\n"
     ]
    }
   ],
   "source": [
    "#Converting word embeddings into tokenized vector\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(headlines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(headlines)\n",
    "\n",
    "#padding sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "headline_pad = pad_sequences(sequences, maxlen= max_length)\n",
    "print('Shape of headline Tensor:', headline_pad.shape)\n",
    "print('Shape of label:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3393\n"
     ]
    }
   ],
   "source": [
    "#Mapping embedding from the word2vec model for each word to the vocabulary and creating a mtrix with word vectors\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in the embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.initializers import Constant\n",
    "\n",
    "#Defining model and using the embeddding  matrix as input to the embedding layer\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                           EMBEDDING_DIM,\n",
    "                           embeddings_initializer= Constant(embedding_matrix),\n",
    "                           input_length= max_length,\n",
    "                           trainable= False)#Setting trainable as false since the embedding is already learned\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32, dropout= 0.2, recurrent_dropout= 0.2))\n",
    "model.add(Dense(1, activation= 'relu'))\n",
    "\n",
    "#Setting model parameters\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 100)            339300    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 352,101\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 339,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model summary with pre-trained embedding\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into a training and a validtion set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = lb_make.fit_transform(labels)\n",
    "\n",
    "X_train_pad_gen, X_test_pad_gen, y_train_gen, y_test_gen = train_test_split(headline_pad, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/25\n",
      " - 5s - loss: 4.4837 - acc: 0.4588 - val_loss: 2.7425 - val_acc: 0.4950\n",
      "Epoch 2/25\n",
      " - 0s - loss: 2.6295 - acc: 0.4588 - val_loss: 2.3825 - val_acc: 0.4950\n",
      "Epoch 3/25\n",
      " - 0s - loss: 2.3391 - acc: 0.4588 - val_loss: 2.1967 - val_acc: 0.4950\n",
      "Epoch 4/25\n",
      " - 0s - loss: 2.1793 - acc: 0.4588 - val_loss: 2.0726 - val_acc: 0.4950\n",
      "Epoch 5/25\n",
      " - 0s - loss: 2.0675 - acc: 0.4588 - val_loss: 1.9774 - val_acc: 0.4950\n",
      "Epoch 6/25\n",
      " - 0s - loss: 1.9769 - acc: 0.4588 - val_loss: 1.8988 - val_acc: 0.4950\n",
      "Epoch 7/25\n",
      " - 0s - loss: 1.9012 - acc: 0.4588 - val_loss: 1.8292 - val_acc: 0.4950\n",
      "Epoch 8/25\n",
      " - 0s - loss: 1.8314 - acc: 0.4588 - val_loss: 1.7659 - val_acc: 0.4950\n",
      "Epoch 9/25\n",
      " - 0s - loss: 1.7688 - acc: 0.4588 - val_loss: 1.7072 - val_acc: 0.4950\n",
      "Epoch 10/25\n",
      " - 0s - loss: 1.7108 - acc: 0.4588 - val_loss: 1.6516 - val_acc: 0.4950\n",
      "Epoch 11/25\n",
      " - 0s - loss: 1.6547 - acc: 0.4588 - val_loss: 1.5994 - val_acc: 0.4950\n",
      "Epoch 12/25\n",
      " - 0s - loss: 1.6024 - acc: 0.4588 - val_loss: 1.5502 - val_acc: 0.4950\n",
      "Epoch 13/25\n",
      " - 0s - loss: 1.5538 - acc: 0.4588 - val_loss: 1.5034 - val_acc: 0.4950\n",
      "Epoch 14/25\n",
      " - 0s - loss: 1.5070 - acc: 0.4588 - val_loss: 1.4591 - val_acc: 0.4950\n",
      "Epoch 15/25\n",
      " - 0s - loss: 1.4628 - acc: 0.4588 - val_loss: 1.4164 - val_acc: 0.4950\n",
      "Epoch 16/25\n",
      " - 0s - loss: 1.4204 - acc: 0.4588 - val_loss: 1.3756 - val_acc: 0.4950\n",
      "Epoch 17/25\n",
      " - 0s - loss: 1.3777 - acc: 0.4588 - val_loss: 1.3371 - val_acc: 0.4950\n",
      "Epoch 18/25\n",
      " - 0s - loss: 1.3398 - acc: 0.4588 - val_loss: 1.2996 - val_acc: 0.4950\n",
      "Epoch 19/25\n",
      " - 0s - loss: 1.3017 - acc: 0.4588 - val_loss: 1.2638 - val_acc: 0.4950\n",
      "Epoch 20/25\n",
      " - 0s - loss: 1.2655 - acc: 0.4588 - val_loss: 1.2293 - val_acc: 0.4950\n",
      "Epoch 21/25\n",
      " - 0s - loss: 1.2309 - acc: 0.4588 - val_loss: 1.1964 - val_acc: 0.4950\n",
      "Epoch 22/25\n",
      " - 0s - loss: 1.1986 - acc: 0.4588 - val_loss: 1.1640 - val_acc: 0.4950\n",
      "Epoch 23/25\n",
      " - 0s - loss: 1.1647 - acc: 0.4588 - val_loss: 1.1325 - val_acc: 0.4950\n",
      "Epoch 24/25\n",
      " - 0s - loss: 1.1329 - acc: 0.4588 - val_loss: 1.1022 - val_acc: 0.4950\n",
      "Epoch 25/25\n",
      " - 0s - loss: 1.1021 - acc: 0.4588 - val_loss: 1.0733 - val_acc: 0.4950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22aeecbea90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "#Training the classification model\n",
    "model.fit(X_train_pad_gen, y_train_gen, batch_size= 128, epochs= 25, validation_data=(X_test_pad_gen, y_test_gen), verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing TweetTokenizer and TfidfVectorizer using n-gram approach\n",
    "from nltk import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1, 3), max_df= 0.75, min_df = 5, tokenizer= tokenizer.tokenize)\n",
    "\n",
    "full_text = list(processed_feature)\n",
    "vectorizer.fit(full_text)\n",
    "#Vectorizing the input feature training set \n",
    "train_vectorized = vectorizer.transform(x_train)\n",
    "test_vectorized = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oladimeji\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=1) \n",
      "\n",
      " Cross-validation mean accuracy: 0.47120600007481395\n",
      " Accuracy on Test set: 0.49\n",
      "----------------------------------------------------------------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False) \n",
      "\n",
      " Cross-validation mean accuracy: 0.4624995324131224\n",
      " Accuracy on Test set: 0.49\n",
      "----------------------------------------------------------------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n",
      "\n",
      " Cross-validation mean accuracy: 0.45748700108480156\n",
      " Accuracy on Test set: 0.48\n",
      "----------------------------------------------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=2, verbose=0, warm_start=False) \n",
      "\n",
      " Cross-validation mean accuracy: 0.4624995324131224\n",
      " Accuracy on Test set: 0.49\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Importing several classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "#Specifying parameters for the classifiers\n",
    "logreg = LogisticRegression(class_weight= 'balanced')\n",
    "ovr = OneVsRestClassifier(logreg)\n",
    "SVM = svm.SVC(C= 1.0, kernel= 'linear', degree = 3, gamma = 'auto')\n",
    "naive = naive_bayes.MultinomialNB()\n",
    "random_forest = RandomForestClassifier(n_estimators=300, random_state=2)\n",
    "\n",
    "text_classifier = [ovr, SVM, naive, random_forest]\n",
    "\n",
    "#Printing Cross-validation and test accuracy to determine better classifier.\n",
    "for clf in text_classifier:\n",
    "    clf.fit(train_vectorized, Y_train)\n",
    "    prediction = clf.predict(test_vectorized)\n",
    "    print(('{} \\n\\n Cross-validation mean accuracy: {}').format(clf, np.mean(cross_val_score(clf, train_vectorized, Y_train, cv=3, scoring = \"accuracy\"))))\n",
    "    print(' Accuracy on Test set: {}'.format(accuracy_score(prediction, Y_test)))\n",
    "    print('----------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying a new approach using max_features and transforming the vectorized features into an array\n",
    "vectorizer = TfidfVectorizer(max_features=2500, min_df=10, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_features_transformed = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the vectorized features array and label encoded labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features_transformed, labels, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=1) \n",
      "\n",
      " Cross-validation mean accuracy: 0.5037500467586877\n",
      " Accuracy on Test set: 0.53\n",
      "\n",
      " Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Even       0.57      0.57      0.57        87\n",
      "   Negative       0.46      0.49      0.47        57\n",
      "   Positive       0.54      0.50      0.52        56\n",
      "\n",
      "avg / total       0.53      0.53      0.53       200\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False) \n",
      "\n",
      " Cross-validation mean accuracy: 0.49996259304978863\n",
      " Accuracy on Test set: 0.515\n",
      "\n",
      " Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Even       0.83      0.52      0.64       138\n",
      "   Negative       0.34      0.51      0.41        41\n",
      "   Positive       0.19      0.48      0.27        21\n",
      "\n",
      "avg / total       0.66      0.52      0.55       200\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) \n",
      "\n",
      " Cross-validation mean accuracy: 0.5199753114128605\n",
      " Accuracy on Test set: 0.525\n",
      "\n",
      " Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Even       0.84      0.51      0.64       142\n",
      "   Negative       0.33      0.48      0.39        42\n",
      "   Positive       0.23      0.75      0.35        16\n",
      "\n",
      "avg / total       0.68      0.53      0.56       200\n",
      "\n",
      "----------------------------------------------------------------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=2, verbose=0, warm_start=False) \n",
      "\n",
      " Cross-validation mean accuracy: 0.49880297759323683\n",
      " Accuracy on Test set: 0.52\n",
      "\n",
      " Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Even       0.71      0.56      0.63       111\n",
      "   Negative       0.43      0.47      0.45        55\n",
      "   Positive       0.31      0.47      0.37        34\n",
      "\n",
      "avg / total       0.57      0.52      0.53       200\n",
      "\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Determining the best classifier\n",
    "for clf in text_classifier:\n",
    "    clf.fit(X_train, y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(('{} \\n\\n Cross-validation mean accuracy: {}').format(clf, np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring = \"accuracy\"))))\n",
    "    print(' Accuracy on Test set: {}'.format(accuracy_score(prediction, y_test)))\n",
    "    print('\\n Classification Report: \\n{}'.format(classification_report(prediction, y_test)))\n",
    "    print('----------------------------------------------------------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
